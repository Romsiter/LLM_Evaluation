# LLM Evaluation
## Introduction
The evaluation process for the Language Model (LLM) generated SQL Queries involves executing these queries against a remote SQL server. This process aims to assess the correctness and reliability of the SQL queries generated by the LLM by comparing them with the expected queries known as Golden Queries.

## Methodology
The evaluate_sql_generation1 method is utilized for evaluating the accuracy and dependability of LLM-generated SQL queries against the expected queries (Golden Queries). This method takes the following inputs:

 User's Question: The query or request provided by the user.
 Context (SQL Schema): The schema of the SQL database against which the queries are executed.
 LLM Generated SQL Query: The SQL query generated by the Language Model in response to the user's question.
## Instructions
To correctly run the code for evaluating LLM-generated SQL queries, follow these steps:
Input your OpenAI's API key.
Write your question in the textual_query variable.

##LLM_Evaluation_PromptInjection
This Notebook contains code which checks for the prompt injection in the database and sanitize the prompt given by user to remove harmful elements using llm_guard.
